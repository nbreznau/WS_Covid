---
title: "Technical Appendix: Social Inequality and Risk Perceptions During a Pandemic"
output:
  html_document: default
---

#### Nate Breznau
#### Hung H.V. Nguyen
#### Lisa Heukamp
#### *University of Bremen



use deaths_longC as the outcome variable for Stage 2




The following files are necessary to run this code

* *cis2.Rdata* - the data from Breznau (2020) https://osf.io/muhdz/
* *covid-stringency-index.csv* -            https://ourworldindata.org/grapher/covid-stringency-index
* *WID_Data_14082020-111941.xlsx* - https://wid.world/data/
* *swiid8_3.Rda* - Solt Gini data

Solt, Frederick. 2020. "Measuring Income Inequality Across Countries and Over Time: The Standardized World Income Inequality Database." Social Science Quarterly [DOI](https://doi.org/10.1111/ssqu.12795)


This is a follow up study of 

Breznau, Nate. 2020. “The Welfare State and Risk Perceptions: The Novel Coronavirus Pandemic and Public Concern in 70 Countries.” *European Societies*. [DOI](https://doi.org/10.1080/14616696.2020.1793215)



```{r setup}

rm(list = ls(all = T))

wda <- "C:/GitHub/WS_Covid" #it already has a 'wd' in this datafile so again reset it here
wd <- wda

wdira <- function(x){
  paste(wd,x, sep = "/")
}
# need pacman and ragg packages
# install.packages("pacman")
# devtools::install_github('r-lib/ragg')

pacman::p_load("dplyr","countrycode","car","ggplot2","jtools","sjPlot","sjmisc","sjlabelled","tidyverse","psych","lavaan","kableExtra","ggrepel","stringi","margins","readxl","foreign", "ragg")

```

### Load data

```{r load_prep, message = F, warning = F}
# Breznau 2020
load(file = wdira("cis2.Rdata"))

wd <- wda #it already has a 'wd' in this datafile so again reset it here

wdir <- wdira

rm(wda, wdira)

# pull out se of concern_self by country
cis_a <- select(cis_a, cow, concern_self_se)
finaldf_Ca <- left_join(finaldf_Ca, cis_a, by = "cow")
finaldf_C <- left_join(finaldf_C, cis_a, by = "cow")

# keep only the df of interest
rm(list=ls()[! ls() %in% c("fig1_2","finaldf_C","finaldf_Ca", "wdir", "wd", "deaths_longCa")])

# government response
gov_resp <- read.csv(file = wdir("second_study/covid-stringency-index.csv"), header = T)

# income concentration
top_inc <- read_xlsx(wdir("second_study/WID_Data_14082020-111941.xlsx"))

# Solt Gini
load(wdir("second_study/swiid8_3.Rda"))

# countries to keep (at least 20 cases)
countries <- as.list(finaldf_Ca$cow)

```

### Figure 1. Theoretical Model

```{r fig1, echo = T}
knitr::include_graphics(wdir("second_study/Fig1.png"))
```


### Figure 2
```{r fig1_2, echo = T}
agg_png(file = wdir("second_study/Fig2.png"), width = 1200, height = 1020,  res = 144)
print(fig1_2)
dev.off()
knitr::include_graphics(wdir("second_study/Fig2.png"))
```
### Data cleaning

#### Deaths May 31st % increase

Data in deaths_longCa taken from Johns Hopkins

Calculate infection severity as 18-day lead deaths increase from April 15th / May 1st to May 31st, and deaths per capita May 31st.


```{r death outcomes}
infect_merge <- as.data.frame(matrix(nrow = 74, ncol = 1))
infect_merge[1:74,1] <- as.numeric(countries)
colnames(infect_merge) <- c("cow")

d1 <- subset(deaths_longCa, date == "2020-04-15", select = c(cow, dead_lead))
d2 <- subset(deaths_longCa, date == "2020-05-01", select = c(cow, dead_lead))
d3 <- subset(deaths_longCa, date == "2020-05-31", select = c(cow, dead_lead, dead_1st_date))

infect_merge <- left_join(infect_merge, d1, by = "cow")
infect_merge <- left_join(infect_merge, d2, by = "cow")
infect_merge <- left_join(infect_merge, d3, by = "cow")

colnames(infect_merge) <- c("cow","dead_lead_apr15","dead_lead_may1","dead_lead_may31","dead_1st_date")

rm(d1,d2,d3)
```

#### Blavatnik Government Data

A severity scale from researchers at Oxford University. We take the severity of 'lockdown' measures by March 15th to allow for a lag for individuals to become aware of the measures and possibly see their impact. Naumann et al ([2020](https://www.uni-mannheim.de/media/Einrichtungen/gip/Corona_Studie/Schwerpunktbericht_Angstempfinden_Mannheimer_Corona_Studie.pdf)) showed that over time, the 'panic' subsides among the public after strong measures are taken. 

```{r clean_blavatnik, warning = F, message = F}

# get country codes
gov_resp$cow <- countrycode(gov_resp$Entity, "country.name", "cown")

# Create a government average severity scale, since 1st death

dlca <- subset(deaths_longCa, date == "2020-01-30")
dlca <- select(dlca, cow, dead_1st_date)
dlca$dead_1st_date <- as.Date(as.character(dlca$dead_1st_date))

gov_resp <- left_join (gov_resp, dlca, by = "cow")

gov_resp <- subset(gov_resp, !is.na(dead_1st_date))

gov_resp$Date = as.Date(gov_resp$Date, "%d-%b-%y")

colnames(gov_resp) <- c("1","2","Date","gov_resp","cow", "dead_1st_date")

gov_resp <- gov_resp %>%
  group_by(cow) %>%
  mutate(drop = ifelse(Date == "2020-03-15", 0, ifelse(dead_1st_date > Date, 1,  0)),
         gov_resp_a = ifelse(Date == "2020-03-15", gov_resp, NA)) %>%
  ungroup()

gov_resp <- subset(gov_resp, drop == 0)
gov_resp <- select(gov_resp, cow, gov_resp, gov_resp_a, dead_1st_date)
gov_respa <- aggregate(gov_resp, by = list(gov_resp$cow), FUN = mean, na.rm = T)


gov_resp <- select(gov_respa, cow, gov_resp, gov_resp_a, dead_1st_date)
rm(gov_respa)

# impute Grenada, Malta and N Macedonia (using Wikipedia info)
# Grenada, strong lockdown, very few cases. Score = 87
# Malta, moderate measures. Score = 70
# N Macedonia, very strong, near total lockdown. Score = 93

gov_resp[72,1] <- 55
gov_resp[72,3] <- 87

gov_resp[73,1] <- 338
gov_resp[73,3] <- 70

gov_resp[74,1] <- 343
gov_resp[74,3] <- 93

# add in dead_1st_date for the three countries just added (note that this command relies on column '4' being dead_1st_date)

gov_resp[72,4] <- deaths_longCa[deaths_longCa$cow == 55 & deaths_longCa$date == "2020-01-22", 4]
gov_resp[73,4] <- deaths_longCa[deaths_longCa$cow == 338 & deaths_longCa$date == "2020-01-22", 4]
gov_resp[74,4] <- deaths_longCa[deaths_longCa$cow == 343 & deaths_longCa$date == "2020-01-22", 4]

# again impute considering length of time

# very recent, same value
gov_resp[72,2] <- 87

gov_resp[73,2] <- 40
gov_resp[74,2] <- 60

# standardize for ease of interpretation

gov_resp$gov_resp_avg <- as.numeric(scale(gov_resp$gov_resp))
gov_resp$gov_resp = as.numeric(scale(gov_resp$gov_resp_a))

gov_resp <- select(gov_resp, -c(gov_resp_a))



```

#### Income Concentration

As a robustness check we include the top 1% income concentration. We also leave the top 10% concentration in the data here although it has more missing values.

```{r clean_ineq, message = F, warning = F}
top_inc <- top_inc %>%
  mutate(cow = countrycode(Country, "country.name", "cown"))

top_inc1 <- subset(top_inc, top_inc$Percentile == "p99p100")
top_inc2 <- subset(top_inc, top_inc$Percentile == "p90p100")
top_inc1 <- subset(top_inc1, !is.na(top_inc1$cow))
top_inc2 <- subset(top_inc2, !is.na(top_inc2$cow))

# Take the mean from 2010-2019 to account for missing data
top_inc1 <- top_inc1[,c(13:22,23)]
top_inc2 <- top_inc2[,c(3:12,23)]
top_inc1$top1 <- rowMeans(top_inc1[,1:10], na.rm = T)
top_inc2$top10 <- rowMeans(top_inc2[,1:10], na.rm = T)
top_inc1 <- select(top_inc1, cow, top1)
top_inc2 <- select(top_inc2, cow, top10)
# china appears multiple times
top_inc1 <- aggregate(top_inc1, by = list(top_inc1$cow), FUN = mean, na.rm = T)
top_inc2 <- aggregate(top_inc2, by = list(top_inc2$cow), FUN = mean, na.rm = T)
rm(top_inc)

```

#### Solt Gini

The Solt data include multiple measures of the Gini for many countries. We take the average score provided by Solt.

```{r clean_solt, warning = F, message = F}
rm(swiid)
swiid_summary$cow <- countrycode(swiid_summary$country, "country.name", "cown")

#some countries do not have data since 2016, take most recent available year
swiid_summary <- swiid_summary %>%
  mutate(year = ifelse(country == "Algeria" & year == 2011 | country == "Brunei" & year == 1981 | country == "Bosnia and Herzegovina" & year == 2015 | country == "Grenada" & year == 2008 | country == "Guatemala" & year == 2014 | country == "Iceland" & year == 2015 | country == "India" & year == 2012 | country == "Japan" & year == 2015 | country == "Morocco" & year == 2014 | country == "Pakistan" & year == 2015 | country == "Philippines" & year == 2015 | country == "South Africa" & year == 2015 | country == "United Arab Emirates" & year == 2008, 2016, year))

# trim Extremes ZAF BRN
swiid_summary$gini_disp <- ifelse(swiid_summary$gini_disp > 50, 49, swiid_summary$gini_disp)


swiid_summary <- subset(swiid_summary, year >= 2016)
swiid_summary <- select(swiid_summary, cow, gini_disp)
swiid_summary <- aggregate(swiid_summary, by = list(swiid_summary$cow), FUN = mean, na.rm = T)
gini_disp <- select(swiid_summary, cow, gini_disp)

rm(swiid_summary)
```



### Merge Data

```{r merge_clean}

df <- left_join(finaldf_Ca, gov_resp, by = "cow")
df <- left_join(df, gini_disp, by = "cow")
df <- left_join(df, top_inc1, by = "cow")
df <- left_join(df, top_inc2, by = "cow")
df <- left_join(df, infect_merge, by = "cow")

# fix Argentina and Indonesia (last top1 observation was 2004)
df$top1 <- ifelse(df$cow == 160, .168, ifelse(df$cow == 850, .085, df$top1))

```

#### Set up first model

Needed for plotting prior to Tables

```{r m1plot}
# adjust risk for severity of outbreak
m1 <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp, data = df)

# predicted values
df$m1p <- predict.lm(m1, df)

# residuals
df$m1r <- df$concern_self - df$m1p 
```


#### Infection Rate

```{r rate_calc}

# There was a change in reporting in Spain and the deaths dropped suddenly on May 6th, adjust for this here

df$dead_lead_may1 <- ifelse(df$cow == 230, 27000, df$dead_lead_may1)
df <- df %>%
  mutate(rate_1 = ifelse(dead_lead_may31 == 0, 0, (dead_lead_may31 - dead_lead_apr15) / (dead_lead_apr15)),
         rate_2 = ifelse(dead_lead_may31 == 0, 0, (dead_lead_may31 - dead_lead_may1) / (dead_lead_may1)),
         rate_1 = ifelse(rate_1 > 3, 3, rate_1),
         rate_2 = ifelse(rate_2 > 3, 3, rate_2), # trim outliers
         rate_1_pc = ifelse(dead_lead_may31 == 0, 0, (dead_lead_may31 - dead_lead_apr15) / pop),
         rate_2_pc = ifelse(dead_lead_may31 == 0, 0, (dead_lead_may31 - dead_lead_may1) / pop),
         deadpc_may18 = dead_lead_may1/pop
  )
```

### Statistics

#### Descriptives

```{r descriptives}


cor <- select(df, concern_self, concern_self_se, days_since_peak, conf_delta, gov_resp, gov_resp_avg, deadpc_may18, rate_1, rate_2, gini_disp, top1, socpolicy, gdp)

corm <- cor %>%
  mutate(concern_selfsd = sd(concern_self, na.rm = T),
         concern_self_sesd = sd(concern_self_se, na.rm = T),
         days_since_peaksd = sd(days_since_peak, na.rm = T),
         conf_deltasd = sd(conf_delta, na.rm = T),
         gov_respsd = sd(gov_resp, na.rm = T),
         intervsd = sd(gov_resp_avg, na.rm = T),
         dead_leadsd = sd(deadpc_may18, na.rm = T),
         rate_1sd = sd(rate_1, na.rm = T),
         rate_2sd = sd(rate_2, na.rm = T),
         gini_dispsd = sd(gini_disp, na.rm = T),
         top1sd = sd(top1, na.rm = T),
         socpolicysd = sd(socpolicy, na.rm = T),
         gdpsd = sd(gdp, na.rm = T),concern_self_min = min(concern_self, na.rm = T),
         concern_self_se_min = min(concern_self_se, na.rm = T),
         days_since_peak_min = min(days_since_peak, na.rm = T),
         conf_delta_min = min(conf_delta, na.rm = T),
         gov_resp_min = min(gov_resp, na.rm = T),
         intervmin = min(gov_resp_avg, na.rm = T),
         dead_leadmin = min(deadpc_may18, na.rm = T),
         rate_1min = min(rate_1, na.rm = T),
         rate_2min = min(rate_2, na.rm = T),
         gini_disp_min = min(gini_disp, na.rm = T),
         top1_min = min(top1, na.rm = T),
         socpolicy_min = min(socpolicy, na.rm = T),
         gdp_min = min(gdp, na.rm = T),
         concern_self_max = max(concern_self, na.rm = T),
         concern_self_se_max = max(concern_self_se, na.rm = T),
         days_since_peak_max = max(days_since_peak, na.rm = T),
         conf_delta_max = max(conf_delta, na.rm = T),
         gov_resp_max = max(gov_resp, na.rm = T),
         intervmax = max(gov_resp_avg, na.rm = T),
         dead_leadmax = max(deadpc_may18, na.rm = T),
         rate_1max = max(rate_1, na.rm = T),
         rate_2max = max(rate_2, na.rm = T),
         gini_disp_max = max(gini_disp, na.rm = T),
         top1_max = max(top1, na.rm = T),
         socpolicy_max = max(socpolicy, na.rm = T),
         gdp_max = max(gdp, na.rm = T),
         n = ifelse(!is.na(top1), 74, 57),
         concern_self = mean(concern_self, na.rm = T),
         concern_self_se = mean(concern_self_se, na.rm = T),
         days_since_peak = mean(days_since_peak, na.rm = T),
         conf_delta = mean(conf_delta, na.rm = T),
         gov_resp = mean(gov_resp, na.rm = T),
         interv = as.numeric(mean(gov_resp_avg, na.rm = T)),
         dead_lead = mean(deadpc_may18, na.rm = T),
         rate_1 = mean(rate_1, na.rm = T),
         rate_2 = mean(rate_2, na.rm = T),
         gini_disp = mean(gini_disp, na.rm = T),
         top1 = mean(top1, na.rm = T),
         socpolicy = mean(socpolicy, na.rm = T),
         gdp = mean(gdp, na.rm = T))

cor2 <- round(corm[1,1:13], 2)
cor2[2,] <- round(corm[1,14:26], 2)
cor2[3,] <- round(corm[1,27:39], 2)
cor2[4,] <- round(corm[1,40:52], 2)
cor2[5,1:13] <- 74
cor2[5,10] <- 57

colnames(cor2) <- c("Risk Perception", "SE of Risk Perception by Country", "Days Since Curve Inflection", "New Cases Past Week","Gov Response Severity, Mar 15", "Gov Response Severity, since 1st death", "COVID-19 Deaths, per capita (k) May 18th", "Infection Rate Apr15-May31", "Infection Rate May1-May31", "Disposable Income Gini", "Top 1% Income Concentration", "Welfare State", "GDP, per capita (k)")

cor2 <- t(cor2)

kable_styling(kable(cor2, col.names = c("Mean", "SD", "Min", "Max", "N")))

```


#### Correlations

```{r corrs}

f1 <- cor(cor, use = "pairwise.complete.obs")


cor1 <- kable(f1, digits = 2, col.names = c("Risk Perception", "SE of Risk Perception by Country", "Days Since Curve Inflection", "New Cases Past Week","Gov Response Severity, Mar 15", "Gov Response Severity, since 1st death", "COVID-19 Deaths, per capita (k) May 18th", "Infection Rate Apr15-May31", "Infection Rate May1-May31", "Disposable Income Gini", "Top 1% Income Concentration", "Welfare State", "GDP, per capita (k)"))

kable_styling(cor1)
```
#### Additional Fig - Cases per country

```{r plot_case_numbers, echo = T}

ggplot(df, aes(y = concern_self_se, x = cases)) +
  geom_point() +
  xlab("Cases Per Country") +
  ylab("Standard Error of the Mean") +
  theme_classic()

# ggplot(df, aes(x = reorder(iso, concern_self), y = concern_self)) + 
#  geom_bar(stat = "identity") +
#  geom_errorbar(aes(ymin = ymin, ymax = ymax))
```
#### Additional Figs - Residuals

This visualizes the relationship between observed and predicted values of risk perceptions and the regression results used for this prediction.


```{r adjusted_risk, echo = T}



# plot fitted v observed
ggplot(df, aes(y=m1p, x=concern_self)) +
  geom_point() +
  geom_text_repel(aes(label=iso), vjust = 1.5) +
  geom_abline(slope=1) +

  xlab("Observed Risk Perceptions") +
  ylab("Predicted Risk Perceptions") +
  theme_classic()

#   geom_smooth(method=lm, se=FALSE) +

mx <- lm(m1r ~ gini_disp, data = df)
summary(mx)
  
```

#### Figure 3

```{r fig3, echo = T}


agg_png(file = wdir("second_study/Fig3.png"), width = 1000, height = 600, res = 144)
ggplot(df, aes(y=m1r, x=gini_disp)) +
  geom_smooth(method=lm, se=T, size = 0.3, color = "gray30") +
  geom_text_repel(aes(label=iso), size = 3, color = "blue4", segment.size = 0.1) +
  xlab("Disposable Income Inequality") +
  ylab("Risk Perceptions (M1 residuals)") +
  labs(title = "Figure 2. Pandemic Risk Perceptions and Economic Inequality", subtitle = "\'Over\' or \'under\' concern explained by disposable income gini stratification", caption = "Plotted residuals from M1 (see Table 1). Where y > 0 countires have higher risk perceptions than predicted \nfrom the outbreak severity and government response, whereas y < 0 are lower than predicted. \nIncome inequality explains about 20% of the variation in over and under-concern across countries.") +
  theme_classic() + 
  theme(
  plot.title = element_text(),
  plot.subtitle = element_text(face = "italic"),
  plot.caption = element_text(size = 9, color = "grey30", vjust = -2.5),
  axis.title.x = element_text(vjust = -0.8),
  axis.title.y = element_text(vjust = 2),
  )
invisible(dev.off())
knitr::include_graphics(wdir("second_study/Fig3.png"))
#    
```

#### Additional Fig - 3 Way Plot

```{r plot9, echo = T}
mid <- 34
# trim gini to get better color display
df$gini_color <- ifelse(df$gini_disp<45, df$gini_disp, 45)

ggplot(data=df, aes(x=conf_delta, y=concern_self, color = gini_color)) +
  geom_point() +
  geom_smooth(method=lm, se=FALSE, color = "gray50", linetype = "dashed") +
  geom_text_repel(aes(label = iso), size = 2.8) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="gray55", high="red", space="Lab") +
  labs(x= "New Case Rate, past week", y = "Risk Perceptions", color = "Income Inequality\n(Gini, post\ntax & transfer)") +
      theme(panel.background = element_rect(fill = "white", colour = "grey50"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.title = element_text(size = 10))
```

## Main Analysis Stage ONE

### Main Regresson Predicting Risk Perceptions

#### Table 1. M1 through M5

```{r reg_gini, echo = T}

#m1x <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp*gov_resp_avg, data = df)

m2 <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + gini_disp, data = df)

# predicted values for sem
df$m2p <- predict.lm(m2, df)

# residuals
df$m2r <- df$concern_self - df$m2p 

m3 <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + socpolicy, data = df)

m4 <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + gdp, data = df)

m5 <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + gini_disp + socpolicy + gdp, data = df)


tab_model(m1, m2, m3, m4, m5, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.loglik = T, show.aic = T, dv.labels = c("M1", "M2","M3", "M4","M5"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Government Intervention", "Disposable Income Inequality", "Welfare State Strength", "GDP Per Capita"), file = wdir("second_study/Tbl1.html"))

tab_model(m1, m2, m3, m4, m5, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.loglik = T, show.aic = T, dv.labels = c("M1", "M2","M3", "M4","M5"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Government Intervention", "Disposable Income Inequality", "Welfare State Strength", "GDP Per Capita"))
```

#### Standardized Coefficients for Table 1

```{r std_reg}
tab_model(m1, m2, m3, m4, m5, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.std = T, dv.labels = c("M1_Z", "M2_Z","M3_Z", "M4_Z","M5_Z"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Government Intervention", "Disposable Income Inequality", "Welfare State Strength", "GDP Per Capita"))
```

#### Additional Table - The Regression behind Fig 3 (formatted)

```{r resid_reg, echo = T}
m3r <- lm(m1r ~ days_since_peak + conf_delta + gov_resp + gini_disp, data = df)

tab_model(m3r, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.std = T, dv.labels = c("M2_resid"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Intervention Severity", "Disposable Income Inequality"), file = wdir("Tbl2.html"))
```

### Robustness Analysis


#### Additional Table - Top 1% instead of Gini

```{r reg_top1, echo = T}
# create dataset with top1 data cases only

dft <- df[(!is.na(df$top1)),]

m1t <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp, data = dft)

m2t <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + top1, data = dft)

m3t <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + socpolicy, data = dft)

m4t <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + gdp, data = dft)

m5t <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + top1 + socpolicy + gdp, data = dft)

tab_model(m1t, m2t, m3t, m4t, m5t, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.loglik = T, show.aic = T, dv.labels = c("M1", "M2","M3", "M4","M5"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Government Intervention", "Top 1% Income Concentration", "Welfare State Strength", "GDP Per Capita"))
```

#### Additional Table - Standardized Results for above

```{r std_reg_top1}
tab_model(m1t, m2t, m3t, m4t, m5t, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.std = T, dv.labels = c("M1_Z", "M2_Z","M3_Z", "M4_Z","M5_Z"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Intervention Severity", "Top 1% Income Concentration", "Welfare State Strength", "GDP Per Capita"))
```

#### Robustness of Coefficient to Sampling Variation in Country-Means

The confidence interval of our regression estimates is based on a sampling distribution across countries, but we have a potentially large source of uncertainty within countries due to the use of an online survey and some very small samples. The online survey problem cannot be solved directly through bootstrapping, but we can asses the robustness of our estimates using the within-country uncertainty. 

There are many countries with small sample sizes (we excluded those with under 20) ranging from 20-50 whereas there are many other countries with over 5,000. This makes for quite an asymmetry in standard error, i.e., the uncertainty associated with each country's mean. Therefore, we bootstrap the estimates by generating random data that follow a normal distribution for each country based on the standard error. Then we run the analysis on each dataset to generate a confidence interval for our estimates that incorporates the within-country standard error of the mean.

```{r robust_sim, message = F, warning = F}
# The standard error takes into account the sample size, so now we create a simulation standard deviation based on the standard error leading to variation in means across 1000 simulations and a normal distribution.



sim_df <- as.data.frame(matrix(nrow = 1000, ncol = 74))
set.seed(91825)
colnames(sim_df) <- c("sim_2", "sim_20",  "sim_55",  "sim_70",  "sim_90",  "sim_92",  "sim_94", "sim_100", "sim_130", "sim_135", "sim_140", "sim_155", "sim_160", "sim_200", "sim_205", "sim_210", "sim_211", "sim_212", "sim_220", "sim_225", "sim_230", "sim_235", "sim_255", "sim_290", "sim_305", "sim_310", "sim_316", "sim_317", "sim_325", "sim_338", "sim_339", "sim_343", "sim_344", "sim_346", "sim_349", "sim_350", "sim_352", "sim_355", "sim_360", "sim_365", "sim_366", "sim_367", "sim_368", "sim_369", "sim_372", "sim_375", "sim_380", "sim_385", "sim_390", "sim_395", "sim_560", "sim_600", "sim_615", "sim_640", "sim_651", "sim_666", "sim_696", "sim_700", "sim_703", "sim_710", "sim_713", "sim_732", "sim_740", "sim_750", "sim_770", "sim_771", "sim_816", "sim_820", "sim_830", "sim_835", "sim_840", "sim_850", "sim_900", "sim_920")

for (c in countries){
  sim_df[paste0("sim_", c)] <- rnorm(1000, mean = finaldf_Ca[finaldf_Ca$cow == c, "concern_self"], sd = finaldf_Ca[finaldf_Ca$cow == c, "concern_self_se"])
}

sim_df_wide <- as.data.frame(t(sim_df))

sim_df_wide$cow <- finaldf_Ca$cow

finaldf_Ca_sim <- left_join(df, sim_df_wide, by = "cow")


```


```{r robust_reg_a, message = F, warning = F, results='hide'}
for (i in 1:1000){
  name <- paste0("lm",i)
  assign(name, lm(finaldf_Ca_sim[,paste0("V",i)] ~ days_since_peak + conf_delta + gov_resp + gini_disp, data = finaldf_Ca_sim))
}

# coefficients

model.list <- mget(grep("lm[0-9]+$", ls(),value=T))
coefs <- lapply(model.list, function(x)coef(x)[5])


# p-values
pvalues<- lapply(model.list, function(x) summary(x)$coefficients[5,4])
```

Approximate beta coefficient

```{r betasiml, include = F}
tab_model(lm113, show.std = T)

# The average coefficient is 0.026 and this is a standardized effect of 0.20
```

```{r robust_reg_b, message = F, warning = F, results='hide'}
rm(list = ls(pattern = "lm"))
rm(model.list)
results_df <- data.frame(matrix(unlist(coefs), nrow=length(coefs), byrow=T))
results_df[,2] <- data.frame(matrix(unlist(pvalues), nrow=length(pvalues), byrow=T))

colnames(results_df) <- c("coeff","p")
rm(coefs)
rm(pvalues)

```

```{r robust_sim2, message = F, warning = F}
# Now repeat with a different seed



sim_df <- as.data.frame(matrix(nrow = 1000, ncol = 74))
set.seed(66666)
colnames(sim_df) <- c("sim_2", "sim_20",  "sim_55",  "sim_70",  "sim_90",  "sim_92",  "sim_94", "sim_100", "sim_130", "sim_135", "sim_140", "sim_155", "sim_160", "sim_200", "sim_205", "sim_210", "sim_211", "sim_212", "sim_220", "sim_225", "sim_230", "sim_235", "sim_255", "sim_290", "sim_305", "sim_310", "sim_316", "sim_317", "sim_325", "sim_338", "sim_339", "sim_343", "sim_344", "sim_346", "sim_349", "sim_350", "sim_352", "sim_355", "sim_360", "sim_365", "sim_366", "sim_367", "sim_368", "sim_369", "sim_372", "sim_375", "sim_380", "sim_385", "sim_390", "sim_395", "sim_560", "sim_600", "sim_615", "sim_640", "sim_651", "sim_666", "sim_696", "sim_700", "sim_703", "sim_710", "sim_713", "sim_732", "sim_740", "sim_750", "sim_770", "sim_771", "sim_816", "sim_820", "sim_830", "sim_835", "sim_840", "sim_850", "sim_900", "sim_920")

for (c in countries){
  sim_df[paste0("sim_", c)] <- rnorm(1000, mean = finaldf_Ca[finaldf_Ca$cow == c, "concern_self"], sd = finaldf_Ca[finaldf_Ca$cow == c, "concern_self_se"])
}

sim_df_wide <- as.data.frame(t(sim_df))

sim_df_wide$cow <- finaldf_Ca$cow

finaldf_Ca_sim <- left_join(df, sim_df_wide, by = "cow")


```


```{r robust_reg2, message = F, warning = F, results='hide'}
for (i in 1:1000){
  name <- paste0("lm",i)
  assign(name, lm(finaldf_Ca_sim[,paste0("V",i)] ~ days_since_peak + conf_delta + gov_resp + gini_disp, data = finaldf_Ca_sim))
}

# coefficients

model.list <- mget(grep("lm[0-9]+$", ls(),value=T))
coefs <- lapply(model.list, function(x)coef(x)[5])


# p-values
pvalues<- lapply(model.list, function(x) summary(x)$coefficients[5,4])


rm(list = ls(pattern = "lm"))
rm(model.list)
results_df2 <- data.frame(matrix(unlist(coefs), nrow=length(coefs), byrow=T))
results_df2[,2] <- data.frame(matrix(unlist(pvalues), nrow=length(pvalues), byrow=T))

colnames(results_df2) <- c("coeff","p")
rm(coefs)
rm(pvalues)

```

```{r plot_robust}
results_df <- bind_rows(results_df, results_df2)
# gen p-value indicator


results_df$group <- ifelse(results_df$p < 0.05, 1, 0)
results_df$group <- as.factor(results_df$group)

ggplot(results_df, aes(x = p, y = coeff, color = group)) +
         geom_point()


```


### Figure 4. Robustness Test of Inequality Effect

```{r densityplot, echo = T}
# mean(results_df$coeff)
# 0.02569374


agg_png(file = wdir("second_study/Fig4.png"), width = 1000, height = 600, res = 144)
ggplot(results_df, aes(x=coeff)) + 
  geom_density() +
  xlab("Effect of Disposable Income Inequality on Risk Perceptions") +
  labs(title = "Figure 3. Robustness of Economic Inequality Effect", subtitle = "Analysis of 2,000 simulated risk perception means \nusing the within-country standard error", caption = "P-values for all 2,000 effects are less than 0.015, with 99.5% of p-values below 0.005.\nCountry case numbers range from 20 to 21,087; mean case numbers 1,452. ") +
  geom_vline(xintercept = 0.02569, linetype="dashed", 
                color = "red", size=1) +
  geom_text(x=0.023, y=10, label="Robust Coefficient \n(average)\n beta = 0.20", color = "red", size = 3) +
    geom_vline(xintercept = 0.03, linetype="dashed", 
                color = "blue", size=1) +
    geom_text(x=0.0326, y=50, label="Observed Coefficient \n(from M2) \nbeta = 0.50", color = "blue", size = 3) +
  theme_classic() +
      theme(
  plot.title = element_text(),
  plot.subtitle = element_text(face = "italic"),
  plot.caption = element_text(size = 9, color = "grey30", vjust = -1.5),
  axis.title.x = element_text(vjust = -0.8),
  axis.title.y = element_blank(),
  )
invisible(dev.off())
knitr::include_graphics(wdir("second_study/Fig4.png"))
```

## Main Analysis Stage TWO

### Regression Using Observed Values

We did not forsee this, but as conf_delta (new cases) measures a type of information the public and media consumes, it is actually a cause of a lower rate of infection, probably because having awareness of a high infection rate leads to behavioral and policy changes. 


#### Rate 2, May 1 to May 20 increase

This is the preferred model where the DV time frame is exogenous

```{r reg2rate_2}
m13 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp , data = df)

# predicted values
df$m13p <- predict.lm(m13, df)

# residuals
df$m13r <- df$rate_2 - df$m13p

m14 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + concern_self, data = df)

m16 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + concern_self + gini_disp, data = df)

m15 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + gini_disp, data = df)

tab_model(m13, m14, m15, m16, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"))

```


Now run as mediation model

```{r sem1}
model <- ' # direct effect
             rate_2 ~ c*gini_disp + days_since_peak + conf_delta + gov_resp 
           # mediator
             concern_self ~ a*gini_disp + days_since_peak + conf_delta + gov_resp 
             rate_2 ~ b*concern_self 
           # indirect effect (a*b)
             ab := a*b
           # total effect
             total := c + (a*b)
         '
fit <- sem(model, data = df)
summary(fit)

```


```{r fig5, echo = T}

# get new residuals from m13

agg_png(file = wdir("second_study/Fig5.png"), width = 1000, height = 600, res = 144)
ggplot(df, aes(y=m13r, x=gini_disp)) +
  geom_smooth(method=lm, formula = y ~ poly(x, 2), se=T, size = 0.3, color = "gray30") +
  geom_text_repel(aes(label=iso), size = 3, color = "blue4", segment.size = 0.1) +
  xlab("Disposable Income Inequality") +
  ylab("Increase in Infections (%),  May 1st-20th") +
  labs(title = "Figure 5. The Spread of COVID-19 and Economic Inequality", subtitle = "Infections not attributable to prior infection rate or government intervention", caption = "\'Increase in Outbreak\' measured as change in COVID-19 deaths per capita, 18-day lead\npredicted residuals from M13 (see Table 2), thus above zero is a greater than expected rate\n after adjusting for severity of outbreak and government measures in April") +
  theme_classic() + 
  theme(
  plot.title = element_text(),
  plot.subtitle = element_text(face = "italic"),
  plot.caption = element_text(size = 9, color = "grey30", vjust = -2.5),
  axis.title.x = element_text(vjust = -0.8),
  axis.title.y = element_text(vjust = 2),
  )
invisible(dev.off())
knitr::include_graphics(wdir("second_study/Fig5.png"))
#    
```

We know that there is a risk of random sampling variation causing disturbance to the results. Therefore we again simulate the variability in the mean risk perceptions by country. 

Get rid of earlier sampling robustness exercise and just do it here for both.



```{r savepoint}
save(df, file = wdir("COVIDiSTRESS/df_gibbs.Rmd"))
```

