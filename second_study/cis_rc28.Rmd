---
title: "Technical Appendix: Social Inequality, Risk Perceptions and Infection"
output:
  html_document: default
---

#### Nate Breznau
#### Hung H.V. Nguyen
#### Lisa Heukamp
#### *University of Bremen



The following files are necessary to run this code

* *cis2.Rdata* - the data from Breznau (2020) https://osf.io/muhdz/
* *covid-stringency-index.csv* -            https://ourworldindata.org/grapher/covid-stringency-index
* *WID_Data_14082020-111941.xlsx* - https://wid.world/data/
* *swiid8_3.Rda* - Solt (2020) Gini data




Breznau (2020) found that the welfare state probably reduces pandemic risk perceptions, but only for certain countries that failed to take strong government intervention measures. We expect that economic inequality (an outcome of the welfare state, and other things) might be a better indicator of what causes risk perceptions beyond what we would expect given the severity of the local outbreak and government interventions. This is the motivation for the following study.




```{r setup}

rm(list = ls(all = T))

wda <- "C:/GitHub/WS_Covid" #it already has a 'wd' in this datafile so again reset it here
wd <- wda

wdira <- function(x){
  paste(wd,x, sep = "/")
}
# need pacman and ragg packages
# install.packages("pacman")
# devtools::install_github('r-lib/ragg')

pacman::p_load("dplyr","countrycode","car","ggplot2","jtools","sjPlot","sjmisc","sjlabelled","tidyverse","psych","lavaan","kableExtra","ggrepel","stringi","margins","readxl","foreign", "ragg","semTable","htmltools","lm.beta")

```

### Load data

```{r load_prep, message = F, warning = F}
# Breznau 2020
load(file = wdira("cis2.Rdata"))

wd <- wda #it already has a 'wd' in this datafile so again reset it here

wdir <- wdira

rm(wda, wdira)

# pull out se of concern_self by country
cis_a <- select(cis_a, cow, concern_self_se)
finaldf_Ca <- left_join(finaldf_Ca, cis_a, by = "cow")
finaldf_C <- left_join(finaldf_C, cis_a, by = "cow")

# keep only the df of interest
rm(list=ls()[! ls() %in% c("fig1_2","finaldf_C","finaldf_Ca", "wdir", "wd", "deaths_longCa")])

# government response
gov_resp <- read.csv(file = wdir("second_study/covid-stringency-index.csv"), header = T)

# income concentration
top_inc <- read_xlsx(wdir("second_study/WID_Data_14082020-111941.xlsx"))

# Solt Gini
load(wdir("second_study/swiid8_3.Rda"))

# countries to keep (at least 20 cases)
countries <- as.list(finaldf_Ca$cow)

```

### Figure 1. Theoretical Model

```{r fig1, echo = T}
knitr::include_graphics(wdir("second_study/Fig1.png"))
```


### Figure 2
```{r fig1_2, echo = T}
agg_png(file = wdir("second_study/Fig2.png"), width = 1200, height = 1020,  res = 144)
print(fig1_2)
dev.off()
knitr::include_graphics(wdir("second_study/Fig2.png"))
```
### Data cleaning

#### Deaths w 18-day Lead, May 1 - May 20th % increase

Data in deaths_longCa taken from Johns Hopkins https://github.com/CSSEGISandData/COVID-19

Measure the spread of infection as the 18-day lead deaths. Measure the 'increase in infection' as the rate of change from May 1st to May 31st.


```{r death outcomes}
infect_merge <- as.data.frame(matrix(nrow = 74, ncol = 1))
infect_merge[1:74,1] <- as.numeric(countries)
colnames(infect_merge) <- c("cow")

d1 <- subset(deaths_longCa, date == "2020-05-01", select = c(cow, dead_lead))
d2 <- subset(deaths_longCa, date == "2020-05-31", select = c(cow, dead_lead, dead_1st_date))

infect_merge <- left_join(infect_merge, d1, by = "cow")
infect_merge <- left_join(infect_merge, d2, by = "cow")

colnames(infect_merge) <- c("cow","dead_lead_may1","dead_lead_may31","dead_1st_date")

rm(d1,d2)
```

#### Blavatnik Government Data

A severity scale from researchers at Oxford University. We take the severity of 'lockdown' measures by March 15th to allow for a lag for individuals to become aware of the measures and possibly see their impact. Naumann et al ([2020](https://www.uni-mannheim.de/media/Einrichtungen/gip/Corona_Studie/Schwerpunktbericht_Angstempfinden_Mannheimer_Corona_Studie.pdf)) showed that over time, the 'panic' subsides among the public after strong measures are taken. 

```{r clean_blavatnik, warning = F, message = F}

# get country codes
gov_resp$cow <- countrycode(gov_resp$Entity, "country.name", "cown")

# Create a government average severity scale, since 1st death

dlca <- subset(deaths_longCa, date == "2020-01-30")
dlca <- select(dlca, cow, dead_1st_date)
dlca$dead_1st_date <- as.Date(as.character(dlca$dead_1st_date))

gov_resp <- left_join (gov_resp, dlca, by = "cow")

gov_resp <- subset(gov_resp, !is.na(dead_1st_date))

gov_resp$Date = as.Date(gov_resp$Date, "%d-%b-%y")

colnames(gov_resp) <- c("1","2","Date","gov_resp","cow", "dead_1st_date")

gov_resp <- gov_resp %>%
  group_by(cow) %>%
  mutate(drop = ifelse(Date == "2020-03-15", 0, ifelse(dead_1st_date > Date, 1,  0)),
         gov_resp_a = ifelse(Date == "2020-03-15", gov_resp, NA)) %>%
  ungroup()

gov_resp <- subset(gov_resp, drop == 0)
gov_resp <- select(gov_resp, cow, gov_resp, gov_resp_a, dead_1st_date)
gov_respa <- aggregate(gov_resp, by = list(gov_resp$cow), FUN = mean, na.rm = T)


gov_resp <- select(gov_respa, cow, gov_resp, gov_resp_a, dead_1st_date)
rm(gov_respa)

# impute Grenada, Malta and N Macedonia (using Wikipedia info)
# Grenada, strong lockdown, very few cases. Score = 87
# Malta, moderate measures. Score = 70
# N Macedonia, very strong, near total lockdown. Score = 93

gov_resp[72,1] <- 55
gov_resp[72,3] <- 87

gov_resp[73,1] <- 338
gov_resp[73,3] <- 70

gov_resp[74,1] <- 343
gov_resp[74,3] <- 93

# add in dead_1st_date for the three countries just added (note that this command relies on column '4' being dead_1st_date)

gov_resp[72,4] <- deaths_longCa[deaths_longCa$cow == 55 & deaths_longCa$date == "2020-01-22", 4]
gov_resp[73,4] <- deaths_longCa[deaths_longCa$cow == 338 & deaths_longCa$date == "2020-01-22", 4]
gov_resp[74,4] <- deaths_longCa[deaths_longCa$cow == 343 & deaths_longCa$date == "2020-01-22", 4]

# again impute considering length of time

# very recent, same value
gov_resp[72,2] <- 87

gov_resp[73,2] <- 40
gov_resp[74,2] <- 60

# standardize for ease of interpretation

gov_resp$gov_resp_avg <- as.numeric(scale(gov_resp$gov_resp))
gov_resp$gov_resp = as.numeric(scale(gov_resp$gov_resp_a))

gov_resp <- select(gov_resp, -c(gov_resp_a))

```

#### Income Concentration

As a robustness check we include the top 1% income concentration. We also leave the top 10% concentration in the data here although it has more missing values.

```{r clean_ineq, message = F, warning = F}
top_inc <- top_inc %>%
  mutate(cow = countrycode(Country, "country.name", "cown"))

top_inc1 <- subset(top_inc, top_inc$Percentile == "p99p100")
top_inc2 <- subset(top_inc, top_inc$Percentile == "p90p100")
top_inc1 <- subset(top_inc1, !is.na(top_inc1$cow))
top_inc2 <- subset(top_inc2, !is.na(top_inc2$cow))

# Take the mean from 2010-2019 to account for missing data
top_inc1 <- top_inc1[,c(13:22,23)]
top_inc2 <- top_inc2[,c(3:12,23)]
top_inc1$top1 <- rowMeans(top_inc1[,1:10], na.rm = T)
top_inc2$top10 <- rowMeans(top_inc2[,1:10], na.rm = T)
top_inc1 <- select(top_inc1, cow, top1)
top_inc2 <- select(top_inc2, cow, top10)
# china appears multiple times
top_inc1 <- aggregate(top_inc1, by = list(top_inc1$cow), FUN = mean, na.rm = T)
top_inc2 <- aggregate(top_inc2, by = list(top_inc2$cow), FUN = mean, na.rm = T)
rm(top_inc)

```

#### Solt Gini

The Solt data include multiple measures of the Gini for many countries. We take the average score provided by Solt.

```{r clean_solt, warning = F, message = F}
rm(swiid)
swiid_summary$cow <- countrycode(swiid_summary$country, "country.name", "cown")

#some countries do not have data since 2016, take most recent available year
swiid_summary <- swiid_summary %>%
  mutate(year = ifelse(country == "Algeria" & year == 2011 | country == "Brunei" & year == 1981 | country == "Bosnia and Herzegovina" & year == 2015 | country == "Grenada" & year == 2008 | country == "Guatemala" & year == 2014 | country == "Iceland" & year == 2015 | country == "India" & year == 2012 | country == "Japan" & year == 2015 | country == "Morocco" & year == 2014 | country == "Pakistan" & year == 2015 | country == "Philippines" & year == 2015 | country == "South Africa" & year == 2015 | country == "United Arab Emirates" & year == 2008, 2016, year))

# trim Extremes ZAF BRN
swiid_summary$gini_disp <- ifelse(swiid_summary$gini_disp > 50, 49, swiid_summary$gini_disp)


swiid_summary <- subset(swiid_summary, year >= 2016)
swiid_summary <- select(swiid_summary, cow, gini_disp)
swiid_summary <- aggregate(swiid_summary, by = list(swiid_summary$cow), FUN = mean, na.rm = T)
gini_disp <- select(swiid_summary, cow, gini_disp)

rm(swiid_summary)
```



#### Merge Data

```{r merge_clean}

df <- left_join(finaldf_Ca, gov_resp, by = "cow")
df <- left_join(df, gini_disp, by = "cow")
df <- left_join(df, top_inc1, by = "cow")
df <- left_join(df, top_inc2, by = "cow")
df <- left_join(df, infect_merge, by = "cow")

# fix Argentina and Indonesia (last top1 observation was 2004)
df$top1 <- ifelse(df$cow == 160, .168, ifelse(df$cow == 850, .085, df$top1))



```

#### Infection Increase (Ratio May 1-31)

```{r rate_calc}

# There was a change in reporting in Spain and the deaths dropped suddenly on May 6th, adjust for this here. 

df$dead_lead_may1 <- ifelse(df$cow == 230, 27000, df$dead_lead_may1)
df <- df %>%
  mutate(rate_2 = ifelse(dead_lead_may31 == 0, 0, (dead_lead_may31 - dead_lead_may1) / (dead_lead_may1)),
         rate_2 = ifelse(rate_2 > 3, 3, rate_2) # trim outliers
  )

# deaths per capita May 1-31
df$newdthpc <- (df$dead_lead_may31 - df$dead_lead_may1)/df$pop
#make per 10,000 instead of per 1,000
df$newdthpc10 <- df$newdthpc*10
df$rate_3 <- df$newdthpc10
```

#### Squared Terms

```{r srts}
df <- df %>%
  mutate(gini_dispR = gini_disp/100, # make gini smaller to keep boundaries reasonable in SEM
         gini_disp2 = gini_dispR^2,
         concern_self2 = concern_self^2,
         gini_disp2C = gini_disp2 - mean(gini_disp2),
         concern_self2C = concern_self2 - mean(concern_self2))
```


### Statistics

#### Set up first models (exploratory models)

M1 Timing + severity of outbreak should predict risk perceptions.
M22 Risk perceptions + risk perceptions-curve should predict deaths.

```{r m1_m21}
# adjust risk for severity of outbreak
m1 <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp, data = df)

# predicted values
df$m1p <- predict.lm(m1, df)

# residuals
df$m1r <- df$concern_self - df$m1p 

m1a <- summary(m1)
m1a <- paste0("Adjusted r-square = ", round(m1a[["r.squared"]],3))

m23 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + concern_self + I(concern_self^2), data = df)


```
#### Descriptives

```{r descriptives}


cor <- select(df, concern_self, concern_self_se, days_since_peak, conf_delta, gov_resp, rate_2, rate_3, gini_disp, top1, socpolicy, gdp)

corm <- cor %>%
  mutate(concern_selfsd = sd(concern_self, na.rm = T),
         concern_self_sesd = sd(concern_self_se, na.rm = T),
         days_since_peaksd = sd(days_since_peak, na.rm = T),
         conf_deltasd = sd(conf_delta, na.rm = T),
         gov_respsd = sd(gov_resp, na.rm = T),
         rate_2sd = sd(rate_2, na.rm = T),
         rate_3sd = sd(rate_3, na.rm = T),
         gini_dispsd = sd(gini_disp, na.rm = T),
         top1sd = sd(top1, na.rm = T),
         socpolicysd = sd(socpolicy, na.rm = T),
         gdpsd = sd(gdp, na.rm = T),concern_self_min = min(concern_self, na.rm = T),
         concern_self_se_min = min(concern_self_se, na.rm = T),
         days_since_peak_min = min(days_since_peak, na.rm = T),
         conf_delta_min = min(conf_delta, na.rm = T),
         gov_resp_min = min(gov_resp, na.rm = T),
         rate_2min = min(rate_2, na.rm = T),
         rate_3min = min(rate_3, na.rm = T),
         gini_disp_min = min(gini_disp, na.rm = T),
         top1_min = min(top1, na.rm = T),
         socpolicy_min = min(socpolicy, na.rm = T),
         gdp_min = min(gdp, na.rm = T),
         concern_self_max = max(concern_self, na.rm = T),
         concern_self_se_max = max(concern_self_se, na.rm = T),
         days_since_peak_max = max(days_since_peak, na.rm = T),
         conf_delta_max = max(conf_delta, na.rm = T),
         gov_resp_max = max(gov_resp, na.rm = T),
         rate_2max = max(rate_2, na.rm = T),
         rate_3max = max(rate_3, na.rm = T),
         gini_disp_max = max(gini_disp, na.rm = T),
         top1_max = max(top1, na.rm = T),
         socpolicy_max = max(socpolicy, na.rm = T),
         gdp_max = max(gdp, na.rm = T),
         n = ifelse(!is.na(top1), 74, 57),
         concern_self = mean(concern_self, na.rm = T),
         concern_self_se = mean(concern_self_se, na.rm = T),
         days_since_peak = mean(days_since_peak, na.rm = T),
         conf_delta = mean(conf_delta, na.rm = T),
         gov_resp = mean(gov_resp, na.rm = T),
         rate_2 = mean(rate_2, na.rm = T),
         rate_3 = mean(rate_3, na.rm = T),
         gini_disp = mean(gini_disp, na.rm = T),
         top1 = mean(top1, na.rm = T),
         socpolicy = mean(socpolicy, na.rm = T),
         gdp = mean(gdp, na.rm = T))

cor2 <- round(corm[1,1:11], 2)
cor2[2,] <- round(corm[1,12:21], 2)
cor2[3,] <- round(corm[1,22:31], 2)
cor2[4,] <- round(corm[1,32:41], 2)
cor2[5,1:11] <- 74
cor2[5,9] <- 57

colnames(cor2) <- c("Risk Perception", "SE of Risk Perception by Country", "Days Since Curve Inflection", "New Cases Past Week","Strength of Gov Intervention", "Increase in Infection (ratio May 1-31)", "Increase in Infection (per capita May 1-31)", "Disposable Income Gini", "Top 1% Income Concentration", "Welfare State", "GDP, per capita (k)")

cor2 <- t(cor2)

kable_styling(kable(cor2, col.names = c("Mean", "SD", "Min", "Max", "N")))

```


#### Correlations

```{r corrs}

f1 <- cor(cor, use = "pairwise.complete.obs")


cor1 <- kable(f1, digits = 2, col.names = c("Risk Perception", "SE of Risk Perception by Country", "Days Since Curve Inflection", "New Cases Past Week","Strength of Gov Intervention", "Increase in Infection (ratio May 1-31)", "Increase in Infection (per capita May 1-31)", "Disposable Income Gini", "Top 1% Income Concentration", "Welfare State", "GDP, per capita (k)"))

kable_styling(cor1)
```
#### Additional Fig - CiS Cases per country

The COVIDiSTRESS (CiS) survey has a huge variance in cases per country

```{r plot_case_numbers, echo = T}

ggplot(df, aes(y = concern_self_se, x = cases)) +
  geom_point() +
  xlab("Cases Per Country") +
  ylab("Standard Error of the Mean") +
  theme_classic()

# ggplot(df, aes(x = reorder(iso, concern_self), y = concern_self)) + 
#  geom_bar(stat = "identity") +
#  geom_errorbar(aes(ymin = ymin, ymax = ymax))
```
#### Additional Figs - Residuals

This visualizes the relationship between observed and predicted values of risk perceptions and the regression results used for this prediction.

This introduces the difference between 'over' and 'under' concern with the Coronavirus on average in a population. 


```{r adjusted_risk, echo = T}



# plot fitted v observed
ggplot(df, aes(y=m1p, x=concern_self)) +
  geom_point() +
  geom_text_repel(aes(label=iso), vjust = 1.5) +
  geom_abline(slope=1) +

  xlab("Observed Risk Perceptions") +
  ylab("Predicted Risk Perceptions") +
  theme_classic()

#   geom_smooth(method=lm, se=FALSE) +

  
```

#### Figure 3

```{r fig3, echo = T}


agg_png(file = wdir("second_study/Fig3.png"), width = 1000, height = 600, res = 144)
ggplot(df, aes(y=m1r, x=gini_disp)) +
  geom_smooth(method=lm, se=T, size = 0.3, color = "gray30") +
  geom_text_repel(aes(label=iso), size = 3, color = "blue4", segment.size = 0.1) +
  xlab("Disposable Income Inequality") +
  ylab("Risk Perceptions (M1 residuals)") +
  labs(title = "", subtitle = "\'Over\' or \'under\' concern explained by income gini") +
  theme_classic() + 
  theme(
  plot.title = element_text(),
  plot.subtitle = element_text(face = "italic"),
  plot.caption = element_text(size = 9, color = "grey30", vjust = -2.5),
  axis.title.x = element_text(vjust = -0.8),
  axis.title.y = element_text(vjust = 2),
  )
invisible(dev.off())
knitr::include_graphics(wdir("second_study/Fig3.png"))
#    
```

#### Additional Fig - 3 Way Plot

```{r plot9, echo = T}
mid <- 34
# trim gini to get better color display
df$gini_color <- ifelse(df$gini_disp<45, df$gini_disp, 45)

ggplot(data=df, aes(x=conf_delta, y=concern_self, color = gini_color)) +
  geom_point() +
  geom_smooth(method=lm, se=FALSE, color = "gray50", linetype = "dashed") +
  geom_text_repel(aes(label = iso), size = 2.8) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="gray55", high="red", space="Lab") +
  labs(x= "New Case Rate, past week", y = "Risk Perceptions", color = "Income Inequality\n(Gini, post\ntax & transfer)") +
      theme(panel.background = element_rect(fill = "white", colour = "grey50"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.title = element_text(size = 10))
```

## Main Analyses

### Predicting Risk Perceptions

#### Table 1. M1 through M5

So far this works to convert html to png https://cloudconvert.com/html-to-png, but we should play around with htmltools package to automate this in the code eventually.

```{r reg_gini, echo = T}

#m1x <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp*gov_resp_avg, data = df)

m2 <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + gini_disp, data = df)

# predicted values for sem
df$m2p <- predict.lm(m2, df)

# residuals
df$m2r <- df$concern_self - df$m2p 

m3 <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + socpolicy, data = df)

m4 <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + gdp, data = df)

m5 <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + gini_disp + socpolicy + gdp, data = df)


tab_model(m1, m2, m3, m4, m5, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.loglik = T, show.aic = T, dv.labels = c("M1", "M2","M3", "M4","M5"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Government Intervention", "Disposable Income Inequality", "Welfare State Strength", "GDP Per Capita"), file = wdir("second_study/Tbl1.html"))

```


#### Standardized Coefficients for Table 1

```{r std_reg}
tab_model(m1, m2, m3, m4, m5, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.std = T, dv.labels = c("M1_Z", "M2_Z","M3_Z", "M4_Z","M5_Z"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Government Intervention", "Disposable Income Inequality", "Welfare State Strength", "GDP Per Capita"))
```

#### Additional Table "M2_resid"- The Regression behind Fig 3

```{r resid_reg, echo = T}
m3r <- lm(m1r ~ days_since_peak + conf_delta + gov_resp + gini_disp, data = df)

tab_model(m3r, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.std = T, dv.labels = c("M2_resid"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Intervention Severity", "Disposable Income Inequality"))
```

#### Additional Table - Top 1% instead of Gini

```{r reg_top1, echo = T}
# create dataset with top1 data cases only

dft <- df[(!is.na(df$top1)),]

m1t <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp, data = dft)

m2t <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + top1, data = dft)

m3t <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + socpolicy, data = dft)

m4t <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + gdp, data = dft)

m5t <- lm(concern_self ~ days_since_peak + conf_delta + gov_resp + top1 + socpolicy + gdp, data = dft)

tab_model(m1t, m2t, m3t, m4t, m5t, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.loglik = T, show.aic = T, dv.labels = c("M11", "M12","M13", "M14","M15"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Government Intervention", "Top 1% Income Concentration", "Welfare State Strength", "GDP Per Capita"))
```



#### Additional Table - Standardized Results for above

```{r std_reg_top1}
tab_model(m1t, m2t, m3t, m4t, m5t, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.std = T, dv.labels = c("M11_Z", "M12_Z","M13_Z", "M14_Z","M15_Z"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Intervention Severity", "Top 1% Income Concentration", "Welfare State Strength", "GDP Per Capita"))
```



### Predicting Infection Increase (ratio), May 1 - May 31

Conf_delta (new cases) measures a type of information the public and media consumes, it is actually a cause of a lower increase ratio of infection, probably because having awareness of a high infection increase leads to behavioral and policy changes. 


#### Additional Table. Infection Increase as DV, May 1 - May 31

These are the analyses that lead to Table 2. Here OLS is our 'first run', but it is biased because it cannot estimate mediation effects and therefore M40-M43 (and M50-M53 with top 1%) are the preferred models due to maximum-likelihood estimation.

```{r reg2rate_3}
m21 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp , data = df)

m22 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + concern_self, data = df)

m23 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + concern_self + I(concern_self^2), data = df)

m24 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp +  gini_disp, data = df)

m25 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + concern_self + gini_disp, data = df)

m26 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + gini_disp + I(gini_disp^2), data = df)

m27 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp +  concern_self + I(concern_self^2) + gini_disp + I(gini_disp^2), data = df)



tab_model(m21, m22, m23, m24, m25, m26, m27, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.loglik = T, show.aic = T, dv.labels = c("M21", "M22","M23", "M24","M25","M26","M27"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Government Intervention", "Risk Perceptions", "Risk Perceptions^2", "Disposable Income Inequality", "Disposable Income Inequality^2"))


```

```{r stdz_m26}
m27_beta <- lm.beta(m27)
m27_beta_concern <- m27_beta[["standardized.coefficients"]][["concern_self"]] + m27_beta[["standardized.coefficients"]][["I(concern_self^2)"]]

m27_beta_gini <- m27_beta[["standardized.coefficients"]][["gini_disp"]] + m27_beta[["standardized.coefficients"]][["I(gini_disp^2)"]]
```

#### Standardized Coefficients M27

Standardized combined effect of term + term^2

Risk Perceptions = `r print(m27_beta_concern)`
Disposable Income Inequality = `r print(m27_beta_gini`)




#### Additional Table. Infection Increase as DV Using Top 1% instead. 

```{r reg2rate_2}
m31 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp , data = dft)

m32 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + concern_self, data = dft)

m33 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + concern_self + I(concern_self^2), data = dft)

m34 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp +  top1, data = dft)

m35 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + concern_self + top1, data = df)

m36 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp + top1 + I(top1^2), data = dft)

m37 <- lm(rate_2 ~ days_since_peak + conf_delta + gov_resp +  concern_self + I(concern_self^2) + top1 + I(top1^2), data = dft)

tab_model(m31, m32, m33, m34, m35, m36, m37, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.loglik = T, show.aic = T, dv.labels = c("M31", "M32","M33", "M34","M35","M36", "M37"), pred.labels = c("Days Since Curve Inflection", "New Case Rate", "Government Intervention", "Risk Perceptions", "Risk Perceptions^2", "Disposable Income Inequality", "Disposable Income Inequality^2"))


```

#### Standardized Coefficients M37

```{r stdz_m37}
m37_beta <- lm.beta(m37)
m37_beta_concern <- m37_beta[["standardized.coefficients"]][["concern_self"]] + m37_beta[["standardized.coefficients"]][["I(concern_self^2)"]]

m37_beta_gini <- m37_beta[["standardized.coefficients"]][["top1"]] + m37_beta[["standardized.coefficients"]][["I(top1^2)"]]
```

Standardized combined effect of term + term^2

Risk Perceptions = `r print(m36_beta_concern)`
Disposable Income Inequality = `r print(m36_beta_gini`)



### Mediation Analysis

SEM squared variables must be constructed by hand. To keep estimates 'under control', it is useful to center the variables.

#### M40 Baseline Model

This is a maximum-likelihood estimated combination of M1 and M23, i.e., a structural equation model.


```{r sem_baseline}
m40 <- '     rate_2 ~ days_since_peak + conf_delta + gov_resp + concern_self + b2*concern_self2  + c1*gini_dispR + c2*gini_disp2
             concern_self ~ days_since_peak + conf_delta + gov_resp + a1*gini_dispR + a2*gini_disp2
           # this is critical because we constructed one out of the other   
             concern_self2 ~ concern_self
           # covariances

           # intercepts
             rate_2 ~ 1
             concern_self ~ 1
           # constraints
             c1 == 0
             c2 == 0
             b2 == 0
             a1 == 0
             a2 == 0
         '
m40fit <- sem(m40, data = df)
summary(m40fit, fit.measures = T)


```

```{r semstdz41, echo = T}
standardizedsolution(m40fit)
```
#### M41

This is a model that is M40 plus linear mediation of inequality by risk perceptions

```{r sem1}
m41 <- '   # direct effect
             rate_2 ~ c1*gini_dispR + c2*gini_disp2 + days_since_peak + conf_delta + gov_resp 
           # mediator
             concern_self ~ a1*gini_dispR + a2*gini_disp2 + days_since_peak + conf_delta + gov_resp 
             rate_2 ~ b1*concern_self + b2*concern_self2
           # this is critical because we constructed one out of the other   
             concern_self2 ~ concern_self
           # covariances

           # intercepts
             rate_2 ~ 1
             concern_self ~ 1
           # constraints
             c2 == 0
             a2 == 0
             b2 == 0
           # indirect effect 
             ab := a1*b1
           # total effect
             total := c1 + a1*b1
         '
m41fit <- sem(m41, data = df)

Tbl4 <- semTable(m41fit, type = "html", print.results = F)
semTable(m41fit, type = "html", print.results = F)
save_html(Tbl4, wdir("second_study/Tbl4.html"), background = "white", libdir = wdir("second_study/"))

knit_print.html(Tbl4)

```

```{r semstdz, echo = T}
standardizedsolution(m41fit)
```

```{r fig4_mediation, echo = T}
knitr::include_graphics(wdir("second_study/Fig4.png"))
```

#### M42 

This adds the squared term for risk perceptions.


```{r sem42, echo = T}
m42 <- '   # direct effect
             rate_2 ~ c1*gini_dispR + c2*gini_disp2 + days_since_peak + conf_delta + gov_resp 
           # mediator
             concern_self ~ a1*gini_dispR + a2*gini_disp2 + days_since_peak + conf_delta + gov_resp 
             rate_2 ~ b1*concern_self + b2*concern_self2
           # this is critical because we constructed one out of the other   
             concern_self2 ~ concern_self
           # covariances

           # intercepts
             rate_2 ~ 1
             concern_self ~ 1
           # constraints
             c2 == 0
             a2 == 0

         '
m42fit <- sem(m42, data = df, meanstructure = T)
summary(m42fit, fit.measures = T)
standardizedsolution(m42fit)
```


#### M43



##### M43 raw estimates

```{r sem43}
m43 <- '   # direct effect
             rate_2 ~ c1*gini_dispR + c2*gini_disp2 + w11*days_since_peak + w12*conf_delta + w13*gov_resp 
           # mediator
             concern_self ~ a1*gini_dispR + a2*gini_disp2 + w1*days_since_peak + w2*conf_delta + w3*gov_resp 
             rate_2 ~ b1*concern_self + b2*concern_self2 
           # this is critical because we constructed one out of the other   
             concern_self2 ~ concern_self
           # intercept naming
             concern_self ~ i1*1
             rate_2 ~ i2*1
           # constraint
             a2 == 0

         '
m43fit <- sem(m43, data = df, meanstructure = T)

summary(m43fit, fit.measures = T)

standardizedsolution(m43fit)

```

##### M43 for prediction/calculation

This strategy follows Hayes and Preacher (2010). The effect is non-linear meaning that it is heterogeneous. There is no single value of the income inequality effect. Hayes and Preacher (2010) suggest estimating an **instantaneous indirect effect** which is actually based on derivatives and then can be plotted as different effects at different levels.

Table 1 in Hayes and Preacher (2010) offers the derivation of the formulas:

Mediation Formulas

$\hat{Y} = i_{2} + c'_{1}X + c'_{2}X^{2} + b_{1}M + b_{2}M^{2}$

$\hat{M} = i_{1} + aX$

Instantaneous Indirect Effects of $X$ on $Y$ through $M$ are derived as (Table 1):

$a(b_{1} + 2b_{2}\hat{M})$)

The only problem is that the effect of X includes a squared term, this squared term must be treated as a covariate (like $W$ in their paper) and then the model should work, changing the squared term's fixed value for each.

Also, to get predicted instantaneous indirect effect we need to remove the covariance of concern_self and gini_disp2 from contaminating the estimates. We fixed gini_disp2 to zero in the equation for concern_self, thus the algorithm find every other possible way to explain this residual correlation before letting it fall into the residual. This likely skews the results, therefore, for prediction purposes we allow all of this correlation to simply fall into the residual thus alighing the instantaneous indirect effect with the predicted values of rate_2

```{r sem43}
m43p <- '   # direct effect
             rate_2 ~ c1*gini_dispR + c2*gini_disp2 + w11*days_since_peak + w12*conf_delta + w13*gov_resp 
           # mediator
             concern_self ~ a1*gini_dispR + a2*gini_disp2 + w1*days_since_peak + w2*conf_delta + w3*gov_resp 
             rate_2 ~ b1*concern_self + b2*concern_self2 
           # this is critical because we constructed one out of the other   
             concern_self2 ~ concern_self
           # to fix the residual variance out of the predictions of the model this is necessary
             concern_self ~~ gini_disp2
           # intercept naming
             concern_self ~ i1*1
             rate_2 ~ i2*1
           # constraint
             a2 == 0
           # instantaneous indir effect calc at values
             x1 := .225
             x2 := .25
             x3 := .275
             x4 := .30
             x5 := .325
             x6 := .35
             x7 := .375
             x8 := .40
             x9 := .425
             x10 := .45
             x11 := .475
             x12 := .50
           # requires fixing covariates and gini_disp2 = 0 
             predm1 := i1+(a1*x1)+w1*.30+w2*0.37+0*w3
             predm2 := i1+(a1*x2)+w1*.30+w2*0.37+0*w3
             predm3 := i1+(a1*x3)+w1*.30+w2*0.37+0*w3
             predm4 := i1+(a1*x4)+w1*.30+w2*0.37+0*w3
             predm5 := i1+(a1*x5)+w1*.30+w2*0.37+0*w3
             predm6 := i1+(a1*x6)+w1*.30+w2*0.37+0*w3
             predm7 := i1+(a1*x7)+w1*.30+w2*0.37+0*w3
             predm8 := i1+(a1*x8)+w1*.30+w2*0.37+0*w3
             predm9 := i1+(a1*x9)+w1*.30+w2*0.37+0*w3
             predm10 := i1+(a1*x10)+w1*.30+w2*0.37+0*w3
             predm11 := i1+(a1*x11)+w1*.30+w2*0.37+0*w3
             predm12 := i1+(a1*x12)+w1*.30+w2*0.37+0*w3
           # instantaneous indirect effects (gini_disp2 must be held constant here)
             theta1 := (b1+2*b2*predm1)*a1
             theta2 := (b1+2*b2*predm2)*a1
             theta3 := (b1+2*b2*predm3)*a1
             theta4 := (b1+2*b2*predm4)*a1
             theta5 := (b1+2*b2*predm5)*a1
             theta6 := (b1+2*b2*predm6)*a1
             theta7 := (b1+2*b2*predm7)*a1
             theta8 := (b1+2*b2*predm8)*a1
             theta9 := (b1+2*b2*predm9)*a1
             theta10 := (b1+2*b2*predm10)*a1
             theta11 := (b1+2*b2*predm11)*a1
             theta12 := (b1+2*b2*predm12)*a1
           # pred values
             predy1 := i2 + c1*x1 + c2*x1*x1 + b1*predm1 + b2*predm1*predm1 + w11*.30+w12*0.37+0*w13
             predy2 := i2 + c1*x2 + c2*x2*x2 + b1*predm2 + b2*predm2*predm2 + w11*.30+w12*0.37+0*w13
             predy3 := i2 + c1*x3 + c2*x3*x3 + b1*predm3 + b2*predm3*predm3 + w11*.30+w12*0.37+0*w13
             predy4 := i2 + c1*x4 + c2*x4*x4 + b1*predm4 + b2*predm4*predm4 + w11*.30+w12*0.37+0*w13
             predy5 := i2 + c1*x5 + c2*x5*x5 + b1*predm5 + b2*predm5*predm5 + w11*.30+w12*0.37+0*w13
             predy6 := i2 + c1*x6 + c2*x6*x6 + b1*predm6 + b2*predm6*predm6 + w11*.30+w12*0.37+0*w13
             predy7 := i2 + c1*x7 + c2*x7*x7 + b1*predm7 + b2*predm7*predm7 + w11*.30+w12*0.37+0*w13
             predy8 := i2 + c1*x8 + c2*x8*x8 + b1*predm8 + b2*predm8*predm8 + w11*.30+w12*0.37+0*w13
             predy9 := i2 + c1*x9 + c2*x9*x9 + b1*predm9 + b2*predm9*predm9 + w11*.30+w12*0.37+0*w13
             predy10 := i2 + c1*x10 + c2*x10*x10 + b1*predm10 + b2*predm10*predm10 + w11*.30+w12*0.37+0*w13
             predy11 := i2 + c1*x11 + c2*x11*x11 + b1*predm11 + b2*predm11*predm11 + w11*.30+w12*0.37+0*w13
             predy12 := i2 + c1*x12 + c2*x12*x12 + b1*predm12 + b2*predm12*predm12 + w11*.30+w12*0.37+0*w13

         '
m43pfit <- sem(m43p, data = df, meanstructure = T)


```

#### Table 2. Hack Tab_Model to Make Identical SEM Tables

```{r tab_sem, results = 'hide'}
# set up frames

m40tab <- matrix(summary(m40fit, fit.measures = T))
m40fits <- as.data.frame(m40tab[[1]])
m40parm <- as.data.frame(m40tab[[2]])
m40parm <- select(m40parm, -c(label, exo))
m41tab <- matrix(summary(m41fit, fit.measures = T))
m41fits <- as.data.frame(m41tab[[1]])
m41parm <- as.data.frame(m41tab[[2]])
m41parm <- select(m41parm, -c(label, exo))
m42tab <- matrix(summary(m42fit, fit.measures = T))
m42fits <- as.data.frame(m42tab[[1]])
m42parm <- as.data.frame(m42tab[[2]])
m42parm <- select(m42parm, -c(label,exo))
m43tab <- matrix(summary(m43fit, fit.measures = T))
m43fits <- as.data.frame(m43tab[[1]])
m43parm <- as.data.frame(m43tab[[2]])
m43parm <- select(m43parm, -c(label, exo))

# combine and name which results go with which models

semtab <- rbind(m40parm,m41parm,m42parm,m43parm)
semtab_labels <- as.data.frame(matrix(nrow = length(semtab[,1]), ncol = 1))
semtab_labels$V1 <- as.list(unlist(strsplit(paste(paste(replicate(length(m40parm[,1]), "m40"), collapse = ","), paste(replicate(length(m41parm[,1]), "m41"), collapse = ","), paste(replicate(length(m42parm[,1]), "m42"), collapse = ","), paste(replicate(length(m43parm[,1]), "m43"), collapse = ","), sep = ","), ",")))

semtab <- cbind(semtab_labels, semtab)
semtab <- semtab %>%
  mutate(est = round(est,3),
         se = round(se,3),
         pvalue = round(pvalue,3),
         stars = ifelse(pvalue>0.1,"", ifelse(pvalue>0.05,"*", ifelse(pvalue>0.01,"**","***"))),
         est = ifelse(rhs == "gini_dispR" | rhs == "gini_disp2", est/100, est),
         se = ifelse(rhs == "gini_dispR" | rhs == "gini_disp2", se/100, se)
  )

semtab <- subset(semtab, op == "~" & lhs!= "concern_self2")


semfits <- round(cbind(m40fits,m41fits,m42fits,m43fits),3)
colnames(semfits) <- c("m40","m41","m42","m43")

# get r-squared
m40r2 <- round(as.data.frame(inspect(m40fit,'r2')),3)
m41r2 <- round(as.data.frame(inspect(m41fit,'r2')),3)
m42r2 <- round(as.data.frame(inspect(m42fit,'r2')),3)
m43r2 <- round(as.data.frame(inspect(m43fit,'r2')),3)

semr2s <- cbind(m40r2,m41r2,m42r2,m43r2)
semr2s <- semr2s[1:2,]
colnames(semr2s) <- c("m40","m41","m42","m43")

semfits <- rbind(semfits,semr2s)

semfits <- semfits[c("rate_2", "concern_self", "aic", "bic", "logl"),]

rm(m40tab, m40fits, m40parm, m41tab, m41fits, m41parm, m42tab, m42fits, m42parm, m43tab, m43fits, m43parm, m40r2,m41r2,m42r2,m43r2, semr2s, semtab_labels)

kable(semtab)
kable(semfits)

```

To create a table identical to the tab_model table we adjust the html code by hand, but hopefully we can automate this in the future.

```{r table2}
knitr::include_graphics(wdir("second_study/Tbl2.html"))
```


#### Figure 5

```{r plotting_data, results = 'hide'}
m43mat <- summary(m43pfit)
m43mat <- as.data.frame(m43mat[["PE"]])
a43 <- m43mat[38:49,]
a43 <- select(a43, rhs)
a43$rhs <- as.numeric(a43$rhs)
colnames(a43) <- c("X_gini")
b43 <- m43mat[50:61,]
b43 <- select(b43, label, est)
colnames(b43) <- c("moderator","pred_M")
c43 <- m43mat[62:73,]
c43 <- select(c43, est, se, z, pvalue)
colnames(c43) <- c("inst_indr_effect","iie_se","iie_z","iie_p")
d43 <- m43mat[74:85,]
d43 <- select(d43, est, se, z, pvalue)
colnames(d43) <- c("pred_Y","Y_se","Y_z","Y_p")

m43mat <- cbind(a43,b43,c43,d43)

# Now transform so that all values are relative to an 'average', axis cross

# 1 - rescale Gini back into its 100-point original scale
m43mat <- m43mat %>%
  mutate(X_gini = round(X_gini*100,1),
         pred_MC = pred_M - mean(pred_M),
         iie_orig = inst_indr_effect/100,
         iie_origC = iie_orig - mean(iie_orig),
         pred_YavgC = 0.33820*pred_Y, # make avg-Y_hat mean = avg-Y mean
         ymin = pred_YavgC - 1.96*Y_se*0.33820,
         ymax = pred_YavgC + 1.96*Y_se*0.33820)# add 95% confidence intervals)



rm(a43,b43,c43,d43)


  
```

```{r plots}
agg_png(file = wdir("second_study/Fig5.png"), width = 800, height = 600, res = 144)
ggplot(data = m43mat, aes(x = X_gini)) +
  geom_abline(intercept = 0, slope = 0, size = 0.25, color = "grey2") +
  geom_ribbon(aes(ymin=ymin,ymax=ymax), fill = "grey70") +
  geom_line(aes(y = pred_YavgC, color = "a"), size = 1) +
  geom_line(aes(y = pred_MC, color = "b"), size = 1) +
  geom_line(aes(y = iie_orig, color = "c"), size = 1) +
  geom_vline(xintercept = 31, linetype = 5, color = "darkgoldenrod4") +
  annotate(geom="label", x=26, y=1.7, label="Increasing inequality =\nLESS infections",
              color="darkgoldenrod4", size = 2.5) +
  annotate(geom="label", x=36, y=1.7, label="Increasing inequality =\nMORE infections",
              color="darkgoldenrod4", size = 2.5) +
  geom_segment(aes(x = 30.5, y = 1.7, xend = 29.5, yend = 1.7),
                  arrow = arrow(length = unit(0.2, "cm"), type = "closed"), colour = "darkgoldenrod4") +
  geom_segment(aes(x = 31.5, y = 1.7, xend = 32.5, yend = 1.7),
                  arrow = arrow(length = unit(0.2, "cm"), type = "closed"), colour = "darkgoldenrod4") +
  scale_color_manual(name = "", labels = c("a" = "Infection Increase (Y)\nMay 1st to May 31st\nratio, predicted", "c" = "Instantaneous\nIndirect Effect\n(rate of change)" , "b" = "Risk Perceptions (M)\nApril, 2020\npredicted"), values = c("a" = "red", "b" = "blue", "c" = "darkgoldenrod4")) +
  scale_x_continuous(breaks = c(25,30,35,40,45,50), expand = c(0,0)) +
  scale_y_continuous(breaks = c(-1,-0.5,0,0.5,1,1.5,2), limits = c(-1,2), expand = c(0,0)) +
  labs(x = "Disposable Income Inequality (X), gini",y = "Estimate") +
  theme(panel.background = element_blank(),
        panel.grid = element_blank(),
        axis.line = element_line(),
        legend.position = "bottom",
        legend.background = element_rect(fill = "#BFD5E3"),
        legend.key = element_rect(fill = "#BFD5E3"),
        plot.margin=unit(c(1,1,0.1,0.3),"cm"))
dev.off()

knitr::include_graphics(wdir("second_study/Fig5.png"))


```










# sheaf coefficient

```{r sheaf tactic}
df <- df %>%
  mutate(gini_sheaf = -0.124451*gini_disp + 0.001946*gini_disp2,
         concern_sheaf = -10.065394*concern_self + 1.184050*concern_self2)

m2sheaf <- lm(concern_sheaf ~ gini_sheaf + days_since_peak + conf_delta + gov_resp, data = df)
m26sheaf <- lm(rate_2 ~ concern_sheaf + gini_sheaf + days_since_peak + conf_delta + gov_resp, data = df)

med1 <- mediate(m2sheaf, m26sheaf, treat = "gini_sheaf", mediator = "concern_sheaf")

summary(med1)
  
  

```



#### Plotting the curvilinear effect

Use standard OLS for easy plotting

```{r sem2plot}
m42ols <- lm(rate_2 ~ gini_disp + I(gini_disp^2) + days_since_peak + conf_delta + gov_resp + concern_self, data = df)

m42plot <- plot_model(m42ols, type = "pred", terms = c("gini_disp"))


m42plot + labs(title = "", x = "Disposable Income Inequality", y = "Infection Increase (ratio), May 1st-20th")
```


```{r fig5, echo = T}


# m22ralt_real

agg_png(file = wdir("second_study/Fig5.png"), width = 1000, height = 600, res = 144)
m42plot +
  #geom_point(data = df, aes(y = m1r, x = gini_disp), inherit.aes = F) +
  geom_text_repel(data = df, aes(y = newdthpc, x = gini_disp, label= iso), inhereit.aes = F, size = 3, color = "blue4", segment.size = 0.1) +
  xlab("Disposable Income Inequality") +
  ylab("Increase in Infections (ratio),  May 1st-20th") +
  labs(title = "Figure 5. The Spread of COVID-19 and Economic Inequality", subtitle = "Infections not attributable to prior infection rate or government intervention", caption = "\'Increase in Outbreak\' measured as change in COVID-19 deaths per capita, 18-day lead\npredicted residuals from M13 (see Table 2), thus above zero is a greater than expected rate\n after adjusting for severity of outbreak and government measures in April") +
  theme_classic() + 
  theme(
  plot.title = element_text(),
  plot.subtitle = element_text(face = "italic"),
  plot.caption = element_text(size = 9, color = "grey30", vjust = -2.5),
  axis.title.x = element_text(vjust = -0.8),
  axis.title.y = element_text(vjust = 2),
  )
invisible(dev.off())
knitr::include_graphics(wdir("second_study/Fig5.png"))
#      geom_smooth(method=lm, formula = y ~ poly(x, 2), se=T, size = 0.3, color = "gray30") +
#geom_point(data = t1, aes(x = mean_time, y = mean_dist), inherit.aes = FALSE). If working, add + geom_label( ... )

```




We know that there is a risk of random sampling variation causing disturbance to the results. Therefore we again simulate the variability in the mean risk perceptions by country. 

Get rid of earlier sampling robustness exercise and just do it here for both.

#### Robustness of Coefficients to Sampling Variation. Simulating Plausible Alternative Values for Country-Means

The confidence interval of our regression estimates is based on a sampling distribution across countries, but we have a potentially large source of uncertainty within countries due to the use of an online survey and some very small samples. The online survey problem cannot be solved directly through bootstrapping, but we can asses the robustness of our estimates using the within-country uncertainty. 

There are many countries with small sample sizes (we excluded those with under 20) ranging from 20-50 whereas there are many other countries with over 5,000. This makes for quite an asymmetry in standard error, i.e., the uncertainty associated with each country's mean. Therefore, we bootstrap the estimates by generating random data that follow a normal distribution for each country based on the standard error. Then we run the analysis on each dataset to generate a confidence interval for our estimates that incorporates the within-country standard error of the mean.

```{r robust_sim, message = F, warning = F, include = F}
# The standard error takes into account the sample size, so now we create a simulation standard deviation based on the standard error leading to variation in means across 1000 simulations and a normal distribution.



sim_df <- as.data.frame(matrix(nrow = 1000, ncol = 74))
set.seed(91825)
colnames(sim_df) <- c("sim_2", "sim_20",  "sim_55",  "sim_70",  "sim_90",  "sim_92",  "sim_94", "sim_100", "sim_130", "sim_135", "sim_140", "sim_155", "sim_160", "sim_200", "sim_205", "sim_210", "sim_211", "sim_212", "sim_220", "sim_225", "sim_230", "sim_235", "sim_255", "sim_290", "sim_305", "sim_310", "sim_316", "sim_317", "sim_325", "sim_338", "sim_339", "sim_343", "sim_344", "sim_346", "sim_349", "sim_350", "sim_352", "sim_355", "sim_360", "sim_365", "sim_366", "sim_367", "sim_368", "sim_369", "sim_372", "sim_375", "sim_380", "sim_385", "sim_390", "sim_395", "sim_560", "sim_600", "sim_615", "sim_640", "sim_651", "sim_666", "sim_696", "sim_700", "sim_703", "sim_710", "sim_713", "sim_732", "sim_740", "sim_750", "sim_770", "sim_771", "sim_816", "sim_820", "sim_830", "sim_835", "sim_840", "sim_850", "sim_900", "sim_920")

for (c in countries){
  sim_df[paste0("sim_", c)] <- rnorm(1000, mean = finaldf_Ca[finaldf_Ca$cow == c, "concern_self"], sd = finaldf_Ca[finaldf_Ca$cow == c, "concern_self_se"])
}

sim_df_wide <- as.data.frame(t(sim_df))

sim_df_wide$cow <- finaldf_Ca$cow

finaldf_Ca_sim <- left_join(df, sim_df_wide, by = "cow")


```


```{r robust_reg_a, message = F, warning = F, results='hide', include = F}
for (i in 1:1000){
  name <- paste0("lm",i)
  assign(name, lm(finaldf_Ca_sim[,paste0("V",i)] ~ days_since_peak + conf_delta + gov_resp + gini_disp, data = finaldf_Ca_sim))
}

# coefficients

model.list <- mget(grep("lm[0-9]+$", ls(),value=T))
coefs <- lapply(model.list, function(x)coef(x)[5])


# p-values
pvalues<- lapply(model.list, function(x) summary(x)$coefficients[5,4])
```

Approximate beta coefficient

```{r betasiml, include = F}
tab_model(lm113, show.std = T)

# The average coefficient is 0.026 and this is a standardized effect of 0.20
```

```{r robust_reg_b, message = F, warning = F, results='hide', include = F}
rm(list = ls(pattern = "lm"))
rm(model.list)
results_df <- data.frame(matrix(unlist(coefs), nrow=length(coefs), byrow=T))
results_df[,2] <- data.frame(matrix(unlist(pvalues), nrow=length(pvalues), byrow=T))

colnames(results_df) <- c("coeff","p")
rm(coefs)
rm(pvalues)

```

```{r robust_sim2, message = F, warning = F, include = F}
# Now repeat with a different seed



sim_df <- as.data.frame(matrix(nrow = 1000, ncol = 74))
set.seed(66666)
colnames(sim_df) <- c("sim_2", "sim_20",  "sim_55",  "sim_70",  "sim_90",  "sim_92",  "sim_94", "sim_100", "sim_130", "sim_135", "sim_140", "sim_155", "sim_160", "sim_200", "sim_205", "sim_210", "sim_211", "sim_212", "sim_220", "sim_225", "sim_230", "sim_235", "sim_255", "sim_290", "sim_305", "sim_310", "sim_316", "sim_317", "sim_325", "sim_338", "sim_339", "sim_343", "sim_344", "sim_346", "sim_349", "sim_350", "sim_352", "sim_355", "sim_360", "sim_365", "sim_366", "sim_367", "sim_368", "sim_369", "sim_372", "sim_375", "sim_380", "sim_385", "sim_390", "sim_395", "sim_560", "sim_600", "sim_615", "sim_640", "sim_651", "sim_666", "sim_696", "sim_700", "sim_703", "sim_710", "sim_713", "sim_732", "sim_740", "sim_750", "sim_770", "sim_771", "sim_816", "sim_820", "sim_830", "sim_835", "sim_840", "sim_850", "sim_900", "sim_920")

for (c in countries){
  sim_df[paste0("sim_", c)] <- rnorm(1000, mean = finaldf_Ca[finaldf_Ca$cow == c, "concern_self"], sd = finaldf_Ca[finaldf_Ca$cow == c, "concern_self_se"])
}

sim_df_wide <- as.data.frame(t(sim_df))

sim_df_wide$cow <- finaldf_Ca$cow

finaldf_Ca_sim <- left_join(df, sim_df_wide, by = "cow")


```


```{r robust_reg2, message = F, warning = F, results='hide', include = F}
for (i in 1:1000){
  name <- paste0("lm",i)
  assign(name, lm(finaldf_Ca_sim[,paste0("V",i)] ~ days_since_peak + conf_delta + gov_resp + gini_disp, data = finaldf_Ca_sim))
}

# coefficients

model.list <- mget(grep("lm[0-9]+$", ls(),value=T))
coefs <- lapply(model.list, function(x)coef(x)[5])


# p-values
pvalues<- lapply(model.list, function(x) summary(x)$coefficients[5,4])


rm(list = ls(pattern = "lm"))
rm(model.list)
results_df2 <- data.frame(matrix(unlist(coefs), nrow=length(coefs), byrow=T))
results_df2[,2] <- data.frame(matrix(unlist(pvalues), nrow=length(pvalues), byrow=T))

colnames(results_df2) <- c("coeff","p")
rm(coefs)
rm(pvalues)

```

```{r plot_robust, include = F}
results_df <- bind_rows(results_df, results_df2)
# gen p-value indicator


results_df$group <- ifelse(results_df$p < 0.05, 1, 0)
results_df$group <- as.factor(results_df$group)

ggplot(results_df, aes(x = p, y = coeff, color = group)) +
         geom_point()


```

### Figure 6. Robustness Test of Inequality Effect

Probably just present as a table or report in text.

```{r densityplot, echo = T, include = F}
# mean(results_df$coeff)
# 0.02569374


agg_png(file = wdir("second_study/Fig6.png"), width = 1000, height = 600, res = 144)
ggplot(results_df, aes(x=coeff)) + 
  geom_density() +
  xlab("Effect of Disposable Income Inequality on Risk Perceptions") +
  labs(title = "Figure 3. Robustness of Economic Inequality Effect", subtitle = "Analysis of 2,000 simulated risk perception means \nusing the within-country standard error", caption = "P-values for all 2,000 effects are less than 0.015, with 99.5% of p-values below 0.005.\nCountry case numbers range from 20 to 21,087; mean case numbers 1,452. ") +
  geom_vline(xintercept = 0.02569, linetype="dashed", 
                color = "red", size=1) +
  geom_text(x=0.023, y=10, label="Robust Coefficient \n(average)\n beta = 0.20", color = "red", size = 3) +
    geom_vline(xintercept = 0.03, linetype="dashed", 
                color = "blue", size=1) +
    geom_text(x=0.0326, y=50, label="Observed Coefficient \n(from M2) \nbeta = 0.50", color = "blue", size = 3) +
  theme_classic() +
      theme(
  plot.title = element_text(),
  plot.subtitle = element_text(face = "italic"),
  plot.caption = element_text(size = 9, color = "grey30", vjust = -1.5),
  axis.title.x = element_text(vjust = -0.8),
  axis.title.y = element_blank(),
  )
invisible(dev.off())
knitr::include_graphics(wdir("second_study/Fig6.png"))
```

```{r savepoint}
save(df, file = wdir("second_study/df_gibbs.Rmd"))
```

## References

Breznau, Nate. 2020. “The Welfare State and Risk Perceptions: The Novel Coronavirus Pandemic and Public Concern in 70 Countries.” *European Societies*. [DOI](https://doi.org/10.1080/14616696.2020.1793215)

Hayes, Andrew F., and Kristopher J. Preacher. 2010. “Quantifying and Testing Indirect Effects in Simple Mediation Models When the Constituent Paths Are Nonlinear.” Multivariate Behavioral Research 45(4):627–60. [Shared Copy](http://quantpsy.org/pubs/hayes_preacher_2010.pdf)

Solt, Frederick. 2020. "Measuring Income Inequality Across Countries and Over Time: The Standardized World Income Inequality Database." Social Science Quarterly [DOI](https://doi.org/10.1111/ssqu.12795)
